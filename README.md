### synthetic_attention

@misc{tay2021synthesizer,
      title={Synthesizer: Rethinking Self-Attention in Transformer Models}, 
      author={Yi Tay and Dara Bahri and Donald Metzler and Da-Cheng Juan and Zhe Zhao and Che Zheng},
      year={2021},
      eprint={2005.00743},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
